"""
Train XGBoost v3
================

–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏ –Ω–∞ 74 features (single timeframe).

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/train_xgboost_v3.py --coin BTC --tf 15m
    python scripts/train_xgboost_v3.py --coin ETH --tf 1h

–†–µ–∑—É–ª—å—Ç–∞—Ç:
    models/xgboost_v3_{coin}_{tf}.json
    models/xgboost_v3_{coin}_{tf}_metadata.json
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import json
import argparse
from pathlib import Path
from datetime import datetime
from sklearn.metrics import (
    f1_score, roc_auc_score, precision_score, 
    recall_score, accuracy_score, confusion_matrix,
    precision_recall_curve
)


def load_data(parquet_path: str) -> tuple:
    """
    –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ parquet.
    
    Returns
    -------
    tuple
        (X, y, feature_names, df)
    """
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞: {parquet_path}")
    df = pd.read_parquet(parquet_path)
    
    print(f"   –†–∞–∑–º–µ—Ä: {len(df):,} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    print(f"   –ü–µ—Ä–∏–æ–¥: {df.index[0]} ‚Äî {df.index[-1]}")
    
    # –†–∞–∑–¥–µ–ª–∏—Ç—å features –∏ target
    if 'target' not in df.columns:
        raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    X = df.drop(columns=['target'])
    y = df['target']
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ inf/nan
    inf_count = np.isinf(X.values).sum()
    nan_count = np.isnan(X.values).sum()
    
    if inf_count > 0 or nan_count > 0:
        print(f"   ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ inf: {inf_count}, nan: {nan_count}")
        # –ó–∞–º–µ–Ω–∏—Ç—å inf –Ω–∞ nan, –∑–∞—Ç–µ–º –∑–∞–ø–æ–ª–Ω–∏—Ç—å
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
    
    print(f"   Features: {X.shape[1]}")
    print(f"   Class balance: {y.mean():.2%} (class 1)")
    
    return X, y, list(X.columns), df


def temporal_split(X, y, train_ratio: float = 0.8) -> tuple:
    """
    –í—Ä–µ–º–µ–Ω–Ω–æ–π split (–±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è).
    
    Returns
    -------
    tuple
        (X_train, X_test, y_train, y_test, split_idx)
    """
    split_idx = int(len(X) * train_ratio)
    
    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]
    
    print(f"\nüìä Split:")
    print(f"   Train: {len(X_train):,} samples ({train_ratio:.0%})")
    print(f"   Test:  {len(X_test):,} samples ({1-train_ratio:.0%})")
    print(f"   Train class 1: {y_train.mean():.2%}")
    print(f"   Test class 1:  {y_test.mean():.2%}")
    
    return X_train, X_test, y_train, y_test, split_idx


def train_model(
    X_train, y_train, 
    X_test, y_test,
    params: dict = None
) -> xgb.XGBClassifier:
    """
    –û–±—É—á–∏—Ç—å XGBoost –º–æ–¥–µ–ª—å.
    
    Parameters
    ----------
    X_train, y_train : –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    X_test, y_test : —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    params : –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns
    -------
    xgb.XGBClassifier
        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å scale_pos_weight –¥–ª—è imbalanced data
    neg_count = (y_train == 0).sum()
    pos_count = (y_train == 1).sum()
    scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0
    
    # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ, –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)
    default_params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': 4,
        'learning_rate': 0.01,
        'n_estimators': 500,
        'min_child_weight': 5,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'scale_pos_weight': scale_pos_weight,
        'random_state': 42,
        'n_jobs': -1,
        'early_stopping_rounds': 50
    }
    
    if params:
        default_params.update(params)
    
    print(f"\nüîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    for k, v in default_params.items():
        if k not in ['n_jobs', 'random_state']:
            print(f"   {k}: {v}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ...")
    
    model = xgb.XGBClassifier(**default_params)
    model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        verbose=50
    )
    
    return model


def evaluate_model(model, X_test, y_test, threshold: float = 0.5) -> dict:
    """
    –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏.
    
    Returns
    -------
    dict
        –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_proba >= threshold).astype(int)
    
    metrics = {
        "auc": roc_auc_score(y_test, y_proba),
        "f1": f1_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred, zero_division=0),
        "recall": recall_score(y_test, y_pred, zero_division=0),
        "accuracy": accuracy_score(y_test, y_pred),
        "threshold": threshold
    }
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    metrics["confusion_matrix"] = cm.tolist()
    metrics["true_negatives"] = int(cm[0, 0])
    metrics["false_positives"] = int(cm[0, 1])
    metrics["false_negatives"] = int(cm[1, 0])
    metrics["true_positives"] = int(cm[1, 1])
    
    return metrics


def find_optimal_threshold(model, X_test, y_test) -> dict:
    """
    –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π threshold –ø–æ F1.
    """
    y_proba = model.predict_proba(X_test)[:, 1]
    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
    
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_idx = np.argmax(f1_scores[:-1])
    
    return {
        "optimal_threshold": float(thresholds[best_idx]),
        "best_f1": float(f1_scores[best_idx]),
        "precision_at_best": float(precision[best_idx]),
        "recall_at_best": float(recall[best_idx])
    }


def save_model(
    model: xgb.XGBClassifier,
    output_path: str,
    feature_names: list,
    metrics: dict,
    threshold_info: dict,
    parquet_path: str,
    coin: str,
    timeframe: str
):
    """
    –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –∏ metadata.
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
    model.save_model(str(output_path))
    print(f"\nüíæ –ú–æ–¥–µ–ª—å: {output_path}")
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å metadata
    metadata = {
        "created_at": datetime.now().isoformat(),
        "coin": coin,
        "timeframe": timeframe,
        "source_data": parquet_path,
        "n_features": len(feature_names),
        "feature_names": feature_names,
        "metrics": metrics,
        "optimal_threshold": threshold_info,
        "model_params": model.get_params()
    }
    
    metadata_path = output_path.parent / (output_path.stem + '_metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2, default=str)
    print(f"üíæ Metadata: {metadata_path}")


def print_results(metrics: dict, threshold_info: dict, criteria: dict):
    """
    –í—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏.
    """
    print("\n" + "="*60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("="*60)
    
    print(f"\n   Threshold 0.5:")
    print(f"   AUC:       {metrics['auc']:.4f}")
    print(f"   F1:        {metrics['f1']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall:    {metrics['recall']:.4f}")
    
    print(f"\n   Optimal threshold ({threshold_info['optimal_threshold']:.3f}):")
    print(f"   F1:        {threshold_info['best_f1']:.4f}")
    print(f"   Precision: {threshold_info['precision_at_best']:.4f}")
    print(f"   Recall:    {threshold_info['recall_at_best']:.4f}")
    
    print(f"\n   Confusion Matrix:")
    print(f"   TN: {metrics['true_negatives']:,}  FP: {metrics['false_positives']:,}")
    print(f"   FN: {metrics['false_negatives']:,}  TP: {metrics['true_positives']:,}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    print("\n" + "="*60)
    print("‚úÖ –ö–†–ò–¢–ï–†–ò–ò")
    print("="*60)
    
    checks = {
        "AUC > 0.60": metrics['auc'] > criteria.get('auc', 0.60),
        "F1 > 0.50": threshold_info['best_f1'] > criteria.get('f1', 0.50),
        "Precision > 0.45": threshold_info['precision_at_best'] > criteria.get('precision', 0.45),
        "Recall > 0.40": threshold_info['recall_at_best'] > criteria.get('recall', 0.40)
    }
    
    all_pass = True
    for name, passed in checks.items():
        status = "‚úÖ" if passed else "‚ùå"
        print(f"   {status} {name}")
        if not passed:
            all_pass = False
    
    print("\n" + "="*60)
    if all_pass:
        print("üéâ –í–°–ï –ö–†–ò–¢–ï–†–ò–ò –ü–†–û–ô–î–ï–ù–´!")
    else:
        print("‚ö†Ô∏è –ù–ï –í–°–ï –ö–†–ò–¢–ï–†–ò–ò –ü–†–û–ô–î–ï–ù–´")
    print("="*60)
    
    return all_pass


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ XGBoost v3')
    parser.add_argument('--coin', type=str, default='BTC', 
                       help='–ú–æ–Ω–µ—Ç–∞ (BTC, ETH, SOL, ...)')
    parser.add_argument('--tf', type=str, default='15m',
                       help='–¢–∞–π–º—Ñ—Ä–µ–π–º (15m, 1h, 4h, 1d)')
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print(f"üöÄ XGBOOST TRAINING v3: {args.coin}/{args.tf}")
    print("="*60)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    PARQUET_PATH = f"data/features/{args.coin}_USDT_{args.tf}_features.parquet"
    OUTPUT_PATH = f"models/xgboost_v3_{args.coin.lower()}_{args.tf}.json"
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if not Path(PARQUET_PATH).exists():
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {PARQUET_PATH}")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        data_dir = Path("data/features")
        if data_dir.exists():
            parquet_files = list(data_dir.glob("*.parquet"))
            for f in sorted(parquet_files)[:10]:
                print(f"   - {f.name}")
        return False
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
    CRITERIA = {
        "auc": 0.60,
        "f1": 0.50,
        "precision": 0.45,
        "recall": 0.40
    }
    
    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
    X, y, feature_names, df = load_data(PARQUET_PATH)
    
    # 2. Split
    X_train, X_test, y_train, y_test, split_idx = temporal_split(X, y, 0.8)
    
    # 3. –û–±—É—á–∏—Ç—å
    model = train_model(X_train, y_train, X_test, y_test)
    
    # 4. –û—Ü–µ–Ω–∏—Ç—å
    metrics = evaluate_model(model, X_test, y_test, threshold=0.5)
    threshold_info = find_optimal_threshold(model, X_test, y_test)
    
    # 5. –í—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    all_pass = print_results(metrics, threshold_info, CRITERIA)
    
    # 6. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
    save_model(
        model=model,
        output_path=OUTPUT_PATH,
        feature_names=feature_names,
        metrics=metrics,
        threshold_info=threshold_info,
        parquet_path=PARQUET_PATH,
        coin=args.coin,
        timeframe=args.tf
    )
    
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    
    return all_pass


if __name__ == "__main__":
    main()
