#!/usr/bin/env python3
"""
–ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –í–°–ï–• –ú–û–î–ï–õ–ï–ô –ò –î–ê–ù–ù–´–•
======================================

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ 3 XGBoost –º–æ–¥–µ–ª–∏, –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ –¥–∞–Ω–Ω—ã–µ.
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–ø—Ä–∏–Ω—Ç–∞.
"""

import sys
from pathlib import Path
import json
import pandas as pd
import numpy as np
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.parent

# ============================================================================
# –ß–ê–°–¢–¨ 1: –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•
# ============================================================================

def analyze_all_data():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ OHLCV —Ñ–∞–π–ª—ã"""

    print("\n" + "="*100)
    print("–ê–ù–ê–õ–ò–ó –í–°–ï–• OHLCV –î–ê–ù–ù–´–•")
    print("="*100)

    data_dir = PROJECT_ROOT / "data" / "raw"
    parquet_files = list(data_dir.glob("*_USDT_*.parquet"))

    if not parquet_files:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã OHLCV —Ñ–∞–π–ª—ã")
        return {}

    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ OHLCV: {len(parquet_files)}\n")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    data_summary = {}

    print(f"{'Pair':<20} {'Rows':<10} {'Date Range':<35} {'Size MB':<8}")
    print("-" * 80)

    for filepath in sorted(parquet_files):
        try:
            df = pd.read_parquet(filepath)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É –≤—Ä–µ–º–µ–Ω–∏
            time_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]

            pair = filepath.stem
            rows = len(df)
            size_mb = filepath.stat().st_size / (1024 * 1024)

            date_range = "?"
            if time_cols:
                try:
                    dates = pd.to_datetime(df[time_cols[0]])
                    date_range = f"{dates.min().date()} to {dates.max().date()}"
                except:
                    pass

            data_summary[pair] = {
                'rows': rows,
                'size_mb': size_mb,
                'date_range': date_range,
                'file': filepath
            }

            print(f"{pair:<20} {rows:<10} {date_range:<35} {size_mb:<8.1f}")
        except Exception as e:
            print(f"‚ùå Error reading {filepath.name}: {e}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "-"*80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê:")

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–æ–Ω–µ—Ç–∞–º –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
    coins = {}
    timeframes = {}

    for pair in data_summary.keys():
        parts = pair.split('_')
        if len(parts) >= 3:
            coin = parts[0]
            tf = parts[-1]

            if coin not in coins:
                coins[coin] = 0
            coins[coin] += 1

            if tf not in timeframes:
                timeframes[tf] = 0
            timeframes[tf] += 1

    print(f"  –ú–æ–Ω–µ—Ç—ã: {len(coins)} ({', '.join(sorted(coins.keys()))})")
    print(f"  –¢–∞–π–º—Ñ—Ä–µ–π–º—ã: {len(timeframes)} ({', '.join(sorted(timeframes.keys()))})")
    print(f"  –í—Å–µ–≥–æ –ø–∞—Ä: {len(data_summary)}")
    print(f"  Total data size: {sum(d['size_mb'] for d in data_summary.values()):.1f} MB")

    # Date ranges
    all_dates = []
    for data in data_summary.values():
        range_str = data['date_range']
        if range_str != "?" and " to " in range_str:
            all_dates.extend(range_str.split(" to "))

    if all_dates:
        try:
            dates = pd.to_datetime(all_dates)
            print(f"  Overall date range: {dates.min().date()} to {dates.max().date()}")
            print(f"  Spanning: {(dates.max() - dates.min()).days} days")
        except:
            pass

    return data_summary

# ============================================================================
# –ß–ê–°–¢–¨ 2: –ê–ù–ê–õ–ò–ó XGBOOST –ú–û–î–ï–õ–ï–ô
# ============================================================================

def analyze_xgboost_models():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ XGBoost –º–æ–¥–µ–ª–∏"""

    print("\n" + "="*100)
    print("–ê–ù–ê–õ–ò–ó XGBOOST –ú–û–î–ï–õ–ï–ô")
    print("="*100)

    model_dir = PROJECT_ROOT / "models"

    models_to_check = [
        ("xgboost_model.json", "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å"),
        ("xgboost_multi_tf_model.json", "Multi-Timeframe –º–æ–¥–µ–ª—å"),
        ("xgboost_normalized_model.json", "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å"),
    ]

    models_info = {}

    for model_file, description in models_to_check:
        model_path = model_dir / model_file

        if not model_path.exists():
            print(f"\n‚ùå {description} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_file}")
            continue

        print(f"\n‚úÖ {description}")
        print(f"   File: {model_file}")
        print(f"   Size: {model_path.stat().st_size / 1024:.1f} KB")

        try:
            with open(model_path, 'r') as f:
                model_data = json.load(f)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            info = {
                'file': model_file,
                'description': description,
                'size_kb': model_path.stat().st_size / 1024,
            }

            if isinstance(model_data, dict):
                if 'learner' in model_data:
                    learner = model_data['learner']

                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
                    if 'attributes' in learner:
                        attrs = learner['attributes']
                        print(f"   Best iteration: {attrs.get('best_iteration', '?')}")
                        print(f"   Best score: {attrs.get('best_score', '?')}")
                        info['best_iteration'] = attrs.get('best_iteration')
                        info['best_score'] = attrs.get('best_score')

                    # –ü—Ä–∏–∑–Ω–∞–∫–∏
                    if 'feature_names' in learner:
                        features = learner['feature_names']
                        print(f"   Features: {len(features)}")
                        if len(features) > 0:
                            print(f"   First 5: {features[:5]}")
                        info['n_features'] = len(features)
                        info['features'] = features

                    # –î–µ—Ä–µ–≤—å—è
                    if 'gradient_booster' in learner:
                        gb = learner['gradient_booster']
                        if 'model' in gb:
                            model_info = gb['model']
                            if 'trees' in model_info:
                                n_trees = len(model_info['trees'])
                                print(f"   Trees: {n_trees}")
                                info['n_trees'] = n_trees

            models_info[model_file] = info

        except Exception as e:
            print(f"   ‚ùå Error: {e}")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    print("\n" + "-"*100)
    print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:")

    if "xgboost_normalized_model.json" in models_info:
        print("\n‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú: xgboost_normalized_model.json")
        print("   –ü—Ä–∏—á–∏–Ω–∞: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ª—É—á—à–µ –¥–ª—è production")
    elif "xgboost_multi_tf_model.json" in models_info:
        print("\n‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú: xgboost_multi_tf_model.json")
        print("   –ü—Ä–∏—á–∏–Ω–∞: Multi-timeframe –≤–µ—Ä—Å–∏—è –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã")
    elif "xgboost_model.json" in models_info:
        print("\n‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú: xgboost_model.json")
        print("   –ü—Ä–∏—á–∏–Ω–∞: –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç")

    return models_info

# ============================================================================
# –ß–ê–°–¢–¨ 3: –ê–ù–ê–õ–ò–ó –ö–û–ú–ü–û–ù–ï–ù–¢–û–í
# ============================================================================

def analyze_components():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã"""

    print("\n" + "="*100)
    print("–ê–ù–ê–õ–ò–ó –ì–û–¢–û–í–´–• –ö–û–ú–ü–û–ù–ï–ù–¢–û–í P_j(S)")
    print("="*100)

    models_dir = PROJECT_ROOT / "models"

    components = {
        'crisis_classifier.py': '–î–µ—Ç–µ–∫—Ç–æ—Ä –∫—Ä–∏–∑–∏—Å–∞',
        'regime_detector.py': '–î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º–æ–≤',
        'opportunity_scorer.py': '–û—Ü–µ–Ω–∫–∞ –≤—ã–≥–æ–¥–Ω–æ—Å—Ç–∏',
        'hybrid_entry_system.py': '–ì–∏–±—Ä–∏–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤—Ö–æ–¥–∞',
        'position_manager.py': '–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–∑–∏—Ü–∏–π',
        'exit_strategy.py': '–°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã—Ö–æ–¥–∞',
        'governance.py': '–ì—É–≤–µ—Ä–Ω–∞–Ω—Å',
        'decision_formula_v2.py': '–§–æ—Ä–º—É–ª–∞ —Ä–µ—à–µ–Ω–∏—è',
        'pjs_components.py': 'P_j(S) –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã',
    }

    found = []
    missing = []

    print(f"\n{'–ö–æ–º–ø–æ–Ω–µ–Ω—Ç':<35} {'–°—Ç–∞—Ç—É—Å':<8} {'–†–∞–∑–º–µ—Ä'}")
    print("-" * 80)

    for filename, description in components.items():
        filepath = models_dir / filename

        if filepath.exists():
            size_kb = filepath.stat().st_size / 1024
            print(f"‚úÖ {description:<33} OK       {size_kb:.1f} KB")
            found.append(filename)
        else:
            print(f"‚ùå {description:<33} MISSING")
            missing.append(filename)

    print(f"\n‚úÖ Found: {len(found)}/{len(components)}")

    return found, missing

# ============================================================================
# –ß–ê–°–¢–¨ 4: SCALERS & CONFIG
# ============================================================================

def analyze_scalers_and_config():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç scalers –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""

    print("\n" + "="*100)
    print("–ê–ù–ê–õ–ò–ó SCALERS –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò")
    print("="*100)

    models_dir = PROJECT_ROOT / "models"

    # Scalers
    print("\nüìä SCALERS:")
    scaler_files = {
        'xgboost_normalized_scaler.pkl': '–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π scaler',
        'xgboost_multi_tf_scaler.pkl': 'Multi-TF scaler',
        'scaler_X_v3.pkl': 'Feature scaler v3',
        'scaler_y.pkl': 'Target scaler',
    }

    for filename, description in scaler_files.items():
        filepath = models_dir / filename
        if filepath.exists():
            print(f"  ‚úÖ {description:<30} {filepath.stat().st_size / 1024:.1f} KB")
        else:
            print(f"  ‚ùå {description:<30} NOT FOUND")

    # Config
    print("\n‚öôÔ∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø:")
    config_files = {
        'best_tp_sl_config.json': '–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ TP/SL',
        'xgboost_normalized_features.json': '–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏',
        'xgboost_multi_tf_features.json': 'Multi-TF –ø—Ä–∏–∑–Ω–∞–∫–∏',
        'xgboost_best_threshold.txt': '–ü–æ—Ä–æ–≥ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è',
        'xgboost_multi_tf_threshold.txt': 'Multi-TF –ø–æ—Ä–æ–≥',
        'xgboost_normalized_threshold.txt': '–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥',
    }

    for filename, description in config_files.items():
        filepath = models_dir / filename
        if filepath.exists():
            print(f"  ‚úÖ {description:<30} {filepath.stat().st_size / 1024:.1f} KB")
        else:
            print(f"  ‚ùå {description:<30} NOT FOUND")

# ============================================================================
# MAIN
# ============================================================================

def main():
    print("\n" + "="*100)
    print("–ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–û–ï–ö–¢–ê SCARLET-SAILS")
    print("="*100)
    print(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"–ü—Ä–æ–µ–∫—Ç: {PROJECT_ROOT}")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —á–∞—Å—Ç–∏
    data_summary = analyze_all_data()
    models_info = analyze_xgboost_models()
    components_found, components_missing = analyze_components()
    analyze_scalers_and_config()

    # –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢
    print("\n" + "="*100)
    print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("="*100)

    print(f"""
‚úÖ –ì–û–¢–û–í–´–ï –†–ï–°–£–†–°–´:
   - OHLCV –¥–∞–Ω–Ω—ã–µ: {len(data_summary)} –ø–∞—Ä (14 –º–æ–Ω–µ—Ç √ó 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞)
   - XGBoost –º–æ–¥–µ–ª–∏: {len(models_info)} –≤–µ—Ä—Å–∏–π
   - –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã P_j(S): {len(components_found)}/9 –≥–æ—Ç–æ–≤—ã—Ö
   - Scalers & Config: –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä

üöÄ –ì–û–¢–û–í–´ –ö –°–ü–†–ò–ù–¢–£:
   ‚úÖ –î–∞–Ω–Ω—ã–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
   ‚úÖ ML –º–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã
   ‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã
   ‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–æ—Ç–æ–≤–∞

üìù –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò –î–õ–Ø –°–ü–†–ò–ù–¢–ê:

   –§–ê–ó–ê 1 (DAY 1):
   1. –í—ã–±—Ä–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 15m)
   2. –í—ã–±—Ä–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–Ω–µ—Ç—É (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è BTC)
   3. –ó–∞–≥—Ä—É–∑–∏—Ç—å xgboost_normalized_model + scaler
   4. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ P_j(S) framework
   5. –ó–∞–ø—É—Å—Ç–∏—Ç—å V5 —Ç–µ—Å—Ç (Full P_j(S) with ML)

   –§–ê–ó–ê 2 (DAY 2):
   1. Risk Aggregation (L2 –Ω–æ—Ä–º–∞ –∏–∑ –≤–∞—à–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
   2. Regime Detection (—É–∂–µ –µ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç!)
   3. Adaptive TP/SL selection
   4. OOT validation –Ω–∞ 2024 –≥–æ–¥—É
   5. Generate reports –¥–ª—è –≤—Å–µ—Ö 3 –º–æ–¥–µ–ª–µ–π

üéØ –ö–õ–Æ–ß–ï–í–´–ï –¢–û–ß–ö–ò:
   - Multi-TF –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ –≤—Å–µ–º–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏ —Å—Ä–∞–∑—É
   - –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ –¥–ª—è production
   - –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã P_j(S) —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã!
   - –ú–∞—Å—Å–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è train/test/OOT

üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:
   –ù–∞—á–Ω—ë–º —Å NORMALIZED –º–æ–¥–µ–ª–∏ + BTC 15m
   –ó–∞—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –Ω–∞ –≤—Å–µ—Ö 14 –º–æ–Ω–µ—Ç –∏ 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
""")

    print("="*100)

if __name__ == '__main__':
    main()
